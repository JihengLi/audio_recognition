{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Parameters"],"metadata":{"id":"P2h1QVJO7bjj"}},{"cell_type":"code","source":["FS = 16000"],"metadata":{"id":"-4uluE8u7-ei"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import and install libraries\n"],"metadata":{"id":"iI5vP4PV7ei_"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"VAaDu5u_V3va"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/Colab\\ Notebooks/project #change to ur dir"],"metadata":{"id":"dBsjLyO5V5MQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","%cd gdrive/MyDrive/audio_recognition/main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YIU7avdWLJmd","executionInfo":{"status":"ok","timestamp":1744602176777,"user_tz":300,"elapsed":2606,"user":{"displayName":"Jiheng Li","userId":"01318088611642014975"}},"outputId":"65b5aa53-e064-4c2a-a731-e912b6669cbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/audio_recognition/main\n"]}]},{"cell_type":"code","source":["# imports:\n","import os\n","import time\n","import fnmatch\n","import glob\n","import random\n","import numpy as np\n","import librosa\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from functools import partial\n","from tqdm import tqdm\n","\n","# PEFT & Whisper related\n","from peft import get_peft_model, LoraConfig\n","from transformers import WhisperModel\n","from transformers import WhisperProcessor\n","\n","# Optional: for argument parsing if args is used (you may already have this in your full code)\n","import argparse"],"metadata":{"id":"lSeQd4vK7-9q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Processing"],"metadata":{"id":"zkM5lpK07hWf"}},{"cell_type":"code","source":["base_dir = \"/content/gdrive/MyDrive/Colab Notebooks/project/Data/genres_original\" #change to ur dir\n","genre_dirs = [os.path.join(base_dir, genre)\n","              for genre in os.listdir(base_dir)\n","              if os.path.isdir(os.path.join(base_dir, genre))]\n","\n","train_files = []\n","val_files = []\n","\n","for genre_dir in genre_dirs:\n","    genre_files = glob.glob(os.path.join(genre_dir, \"*.wav\"))\n","    print(f\"Found {len(genre_files)} files in {os.path.basename(genre_dir)}\")\n","    random.seed(42)\n","    random.shuffle(genre_files)\n","    split_idx = int(0.8 * len(genre_files))\n","    train_files.extend(genre_files[:split_idx])\n","    val_files.extend(genre_files[split_idx:])\n","\n","print(f\"Total train files: {len(train_files)}\")\n","print(f\"Total validation files: {len(val_files)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpZuwSBBLA9k","executionInfo":{"status":"ok","timestamp":1744603716775,"user_tz":300,"elapsed":28,"user":{"displayName":"Jiheng Li","userId":"01318088611642014975"}},"outputId":"ca45e45b-6abd-4f84-aea1-145a720d7c51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 100 files in pop\n","Found 100 files in reggae\n","Found 100 files in country\n","Found 100 files in rock\n","Found 100 files in classical\n","Found 100 files in disco\n","Found 100 files in blues\n","Found 100 files in hiphop\n","Found 100 files in jazz\n","Found 100 files in metal\n","Total train files: 800\n","Total validation files: 200\n"]}]},{"cell_type":"code","source":["def load_audio(file_path, sr=16000):\n","    try:\n","        audio, _ = librosa.load(file_path, sr=sr, mono=True)\n","    except Exception as e:\n","        print(f\"Error loading {file_path}: {e}\")\n","        audio = None\n","    return audio"],"metadata":{"id":"E3ta0AuIblFm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = []\n","for f in train_files:\n","    label = os.path.basename(f).split('.')[0]\n","    audio_array = load_audio(f)\n","    if audio_array is not None:\n","        train_data.append({\"array\": audio_array, \"label\": label})\n","\n","val_data = []\n","for f in val_files:\n","    label = os.path.basename(f).split('.')[0]\n","    audio_array = load_audio(f)\n","    if audio_array is not None:\n","        val_data.append({\"array\": audio_array, \"label\": label})\n","\n","print(f\"Train data samples: {len(train_data)}\")\n","print(f\"Validation data samples: {len(val_data)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8cgNsl1TiBh","executionInfo":{"status":"ok","timestamp":1744603766638,"user_tz":300,"elapsed":22607,"user":{"displayName":"Jiheng Li","userId":"01318088611642014975"}},"outputId":"9e282781-fc75-4959-ea1b-d1480bc9289d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-39-4d6a07a8aabc>:3: UserWarning: PySoundFile failed. Trying audioread instead.\n","  audio, _ = librosa.load(file_path, sr=sr, mono=True)\n","/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"]},{"output_type":"stream","name":"stdout","text":["Error loading ../dataset/GTZAN/genres_original/jazz/jazz.00054.wav: \n","Train data samples: 799\n","Validation data samples: 200\n"]}]},{"cell_type":"code","source":["audio, _ = librosa.load(train_files[:2][1], sr=16000, mono=True)\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n","processor([audio], sampling_rate=16000, return_tensors=\"pt\")"],"metadata":{"id":"e3cc6IaE7_YC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744603779484,"user_tz":300,"elapsed":801,"user":{"displayName":"Jiheng Li","userId":"01318088611642014975"}},"outputId":"5725e3cd-f6dc-4c3d-d225-f8b5e211bfda"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_features': tensor([[[ 0.3245, -0.1991,  0.3658,  ...,  1.0984,  1.1399,  1.0821],\n","         [ 0.5686,  0.5699,  0.5394,  ...,  0.9421,  1.1574,  0.9613],\n","         [ 0.8102,  0.7244,  0.6339,  ...,  0.9872,  1.0744,  0.9841],\n","         ...,\n","         [-0.0594, -0.2086, -0.2773,  ...,  0.2655,  0.2060,  0.0535],\n","         [-0.0990, -0.3853, -0.3440,  ...,  0.1312,  0.0165,  0.0591],\n","         [ 0.0312, -0.3994, -0.4800,  ...,  0.0150, -0.0579, -0.0040]]])}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["class AudioDataset(Dataset):\n","    def __init__(self, data, processor, label_to_idx, sr=16000):\n","        self.data = data\n","        self.processor = processor\n","        self.label_to_idx = label_to_idx\n","        self.sr = sr\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx]\n","        processed = self.processor(sample[\"array\"],\n","                                   sampling_rate=self.sr,\n","                                   return_tensors=\"pt\")\n","        input_features = processed.input_features.squeeze(0)\n","        label = self.label_to_idx[sample[\"label\"]]\n","        return {\"input_features\": input_features, \"label\": label}\n","\n","def collate_fn(batch):\n","    inputs = [item[\"input_features\"] for item in batch]\n","    labels = [item[\"label\"] for item in batch]\n","    inputs = torch.stack(inputs)\n","    labels = torch.tensor(labels, dtype=torch.long)\n","    return inputs, labels"],"metadata":{"id":"WcJyNKk2Ps3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre_dict = {\n","    \"pop\": 0,\n","    \"reggae\": 1,\n","    \"country\": 2,\n","    \"rock\": 3,\n","    \"classical\": 4,\n","    \"disco\": 5,\n","    \"blues\": 6,\n","    \"hiphop\": 7,\n","    \"jazz\": 8,\n","    \"metal\": 9\n","}\n","\n","train_dataset = AudioDataset(train_data, processor, genre_dict)\n","val_dataset = AudioDataset(val_data, processor, genre_dict)\n","\n","train_dataloader = DataLoader(train_dataset,\n","                              batch_size=32,\n","                              shuffle=True,\n","                              collate_fn=collate_fn)\n","\n","validation_dataloader = DataLoader(val_dataset,\n","                                   batch_size=32,\n","                                   shuffle=False,\n","                                   collate_fn=collate_fn)"],"metadata":{"id":"Fik0XLuOT3tD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Models and Loss Functions"],"metadata":{"id":"YZyZc1YV7qvI"}},{"cell_type":"code","source":["class audio_classifier(nn.Module):\n","    def __init__(self, encoder, num_classes=10):\n","        super().__init__()\n","        self.encoder = encoder\n","        hidden_dim = encoder.config.d_model\n","        self.classifier = nn.Sequential(\n","            nn.Linear(hidden_dim, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        with torch.no_grad():\n","            x = self.encoder(x).last_hidden_state\n","        x = x.mean(dim=1)  # [batch, seq_len, hidden_dim] -> [batch, hidden_dim]\n","        return self.classifier(x)\n","\n","class EarlyStopper:\n","    def __init__(self, patience=1, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.min_validation_loss = float('inf')\n","\n","    def early_stop(self, validation_loss):\n","        if validation_loss < self.min_validation_loss:\n","            self.min_validation_loss = validation_loss\n","            self.counter = 0\n","        elif validation_loss > (self.min_validation_loss + self.min_delta):\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False"],"metadata":{"id":"5ks1iYRX7_4a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"R88xKGtq75Hn"}},{"cell_type":"code","source":["from collections import Counter\n","def save_models(results_path, peft_model, dense_layers, lora_weights_path, dense_layers_path):\n","    peft_model.to(\"cpu\")\n","    dense_layers.to(\"cpu\")\n","\n","    peft_model.save_pretrained(results_path + 'models/' + lora_weights_path)\n","    torch.save(dense_layers.state_dict(), results_path + 'models/' + dense_layers_path)\n","\n","def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs, results_path, checkpoint_path, model_name, writer, save_paths):\n","    model.to(device)\n","    criterion = criterion.to(device)\n","    best_val_loss = float('inf')\n","    early_stopper = EarlyStopper(patience=15)\n","\n","    train_losses = []\n","    val_losses = []\n","    val_aucs = []\n","    train_times = []\n","\n","    best_lora_weights_path = os.path.join(results_path, f\"best_{save_paths['lora_weights']}\")\n","    best_dense_layers_path = os.path.join(results_path, f\"best_{save_paths['dense_layers']}\")\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        start_time = time.time()\n","\n","        train_loss = 0.0\n","        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\"):\n","            inputs = inputs.to(device)\n","\n","            #labels = labels.view(-1, 1).float().to(device)\n","            labels = labels.view(-1).long().to(device)\n","\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        train_loss /= len(train_loader)\n","        train_losses.append(train_loss)\n","\n","        end_time = time.time()\n","        epoch_time = end_time - start_time\n","        train_times.append(epoch_time)\n","\n","        eval_out = evaluate(model, val_loader, device, criterion)\n","        val_loss = eval_out['loss']\n","        val_auc = eval_out['auc']\n","\n","        val_losses.append(val_loss)\n","        val_aucs.append(val_auc)\n","\n","        writer.add_scalar(f'{model_name}/train_loss', train_loss, epoch)\n","        writer.add_scalar(f'{model_name}/val_loss', val_loss, epoch)\n","        writer.add_scalar(f'{model_name}/val_auc', val_auc, epoch)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), checkpoint_path)\n","            model.encoder.save_pretrained(best_lora_weights_path)\n","            torch.save(model.classifier.state_dict(), best_dense_layers_path)\n","\n","        if early_stopper.early_stop(val_loss):\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    save_models(\n","        results_path,\n","        peft_model=model.encoder,\n","        dense_layers=model.classifier,\n","        lora_weights_path=save_paths['lora_weights'],\n","        dense_layers_path=save_paths['dense_layers']\n","    )\n","\n","    return train_losses, val_losses, val_aucs, train_times\n","\n","def main(log_dir, train_data, valid_data, batch_size, num_workers, num_class, num_epochs, results_path, method=\"frozen_encoder\"):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    writer = SummaryWriter(log_dir=log_dir)\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn)\n","    valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn)\n","    print(\"Train labels:\", Counter([y for _, y in train_loader]))\n","    print(\"Val   labels:\",   Counter([y for _, y in valid_loader]))\n","\n","    whisper_model = WhisperModel.from_pretrained(\"openai/whisper-tiny\")\n","    whisper_model = whisper_model.encoder\n","\n","    module_names = [name for name, module in whisper_model.named_modules()]\n","\n","    patterns = [\"layers.*.self_attn.q_proj\", \"layers.*.self_attn.k_proj\", \"layers.*.self_attn.v_proj\", \"layers.*.self_attn.o_proj\"]\n","\n","    # Fetching all strings that match the patterns\n","    matched_modules = []\n","    for pattern in patterns:\n","        matched_modules.extend(fnmatch.filter(module_names, pattern))\n","\n","    lora_config = LoraConfig(use_dora=True, r=8, lora_alpha=32, target_modules=matched_modules)\n","    whisper_model_with_lora = get_peft_model(whisper_model, lora_config).to(device)\n","\n","    for name, param in whisper_model_with_lora.named_parameters():\n","        param.requires_grad = 'lora' in name\n","\n","    models = []\n","    encoder = WhisperModel.from_pretrained(f\"openai/whisper-{args.encoder}\").encoder.to(device)\n","\n","    if method == 'frozen_encoder':\n","        models.append(('frozen_encoder', audio_classifier(encoder, num_class)))\n","    elif method == 'lora':\n","        models.append(('lora', audio_classifier(encoder, num_class)))\n","\n","    for model_name, model in models:\n","        if model_name in ['frozen_encoder', 'lora']:\n","            for param in model.encoder.parameters():\n","                param.requires_grad = False\n","\n","\n","        model.to(device)\n","\n","        criterion = nn.CrossEntropyLoss().to(device)\n","        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.learning_rate, betas=(0.9, 0.999), eps=1e-08)\n","        save_paths = {\n","            'lora_weights': f'lora_weights_8_32',\n","            'dense_layers': f'dense_layers_8_32.pth'\n","        }\n","\n","\n","        print(f\"Training {model_name} model from scratch...\")\n","        train_losses, val_losses, val_aucs, train_times = train(\n","        model, train_loader, valid_loader, optimizer, criterion, device, num_epochs, results_path,\n","        f'{model_name}_checkpoint.pt', model_name, writer, save_paths)\n","\n","\n","\n","        plt.figure()\n","        plt.rc('font', family='serif')\n","        plt.plot(train_losses, label='Train Loss')\n","        plt.plot(val_losses, label='Validation Loss')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","#        plt.title(f'{model_name} Loss vs Epoch')\n","        plt.savefig(f'{args.results_path}/figures/{model_name}_loss_lr-8e-4.png')\n","\n","        plt.figure()\n","        plt.rc('font', family='serif')\n","        plt.plot(np.cumsum(train_times), train_losses, label='Train Loss')\n","        plt.plot(np.cumsum(train_times), val_losses, label='Validation Loss')\n","        plt.xlabel('Time (s)')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.title(f'{model_name} Loss vs Time')\n","        plt.savefig(f'{args.results_path}/figures/{model_name}_loss_time_lr-8e-4.png')\n","\n","        plt.figure()\n","        plt.rc('font', family='serif')\n","        plt.plot(val_aucs)\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Validation AUC')\n","#        plt.title(f'{model_name} Val AUC vs Epoch')\n","        plt.savefig(f'{args.results_path}/figures/{model_name}_val_auc_lr-8e-4.png')\n","\n","        plt.figure()\n","        plt.rc('font', family='serif')\n","        plt.plot(np.cumsum(train_times), val_aucs)\n","        plt.xlabel('Time (s)')\n","        plt.ylabel('Validation AUC')\n","        plt.title(f'{model_name} Val AUC vs Time')\n","        plt.savefig(f'{args.results_path}/figures/{model_name}_val_auc_time_lr-8e-4.png')\n","\n","    writer.close()"],"metadata":{"id":"U_C8PMed8AWu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse\n","\n","args = argparse.Namespace(\n","    log_dir=\"./runs/audio_exp1\",\n","    results_path=\"./results/\",\n","    encoder=\"tiny\",\n","    learning_rate=8e-4,\n","    batch_size=16,\n","    num_epochs=20,\n","    num_workers=0,\n","    method=\"frozen_encoder\"\n",")"],"metadata":{"id":"RS9JhpUJWXiM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import label_binarize\n","def evaluate(model, data_loader, device, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    all_logits = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs = inputs.to(device)\n","            labels = labels.view(-1).long().to(device)  # (batch,)\n","\n","            logits = model(inputs)                     # (batch, num_classes)\n","            loss   = criterion(logits, labels)\n","            total_loss += loss.item() * inputs.size(0)\n","\n","            all_logits.append(logits.cpu().numpy())    # list of (batch, num_classes)\n","            all_labels.append(labels.cpu().numpy())    # list of (batch,)\n","\n","    avg_loss = total_loss / len(data_loader.dataset)\n","\n","    all_logits = np.concatenate(all_logits, axis=0)  # (N, num_classes)\n","    all_labels = np.concatenate(all_labels, axis=0)  # (N,)\n","\n","    y_true_oh = label_binarize(all_labels, classes=np.arange(10))  # (N, num_classes)\n","\n","    y_prob = F.softmax(torch.from_numpy(all_logits), dim=1).numpy()         # (N, num_classes)\n","\n","    auc = roc_auc_score(\n","        y_true_oh,\n","        y_prob,\n","        multi_class='ovr',    # one-vs-rest\n","        average='macro'       # macro 平均\n","    )\n","\n","    return {\"loss\": avg_loss, \"auc\": auc}"],"metadata":{"id":"7OKa5LFZWYcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","main(\n","    log_dir=\"/content/gdrive/MyDrive/Colab Notebooks/project/runs/audio_exp1\",#change to ur dir\n","    train_data=train_dataset,\n","    valid_data=val_dataset,\n","    batch_size=16,\n","    num_workers=0,\n","    num_class=10,\n","    num_epochs=20,\n","    results_path=\"/content/gdrive/MyDrive/Colab Notebooks/project/results\", #change to ur dir\n","    method=\"frozen_encoder\"\n",")"],"metadata":{"id":"LTsLrBoPWb-O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After file generated, change the file \"config.json\", \"model.safetensors\" to \"adapter_config.json\", \"adapter_model.safetensors\""],"metadata":{"id":"iHxKgMVBWz2q"}},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"D12Hlubc77-V"}},{"cell_type":"code","source":["!pip install datasets transformers evaluate"],"metadata":{"id":"HXeMUSe2WfUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt-get install libav-tools"],"metadata":{"id":"YCkPwUo3WjKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","fma    = load_dataset(\"rpmon/fma-genre-classification\")\n","val_ds = fma[\"validation\"]\n","fma_genres = val_ds.features[\"genre\"].names"],"metadata":{"id":"4OK5mJnJWg1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fma_genres = val_ds.features[\"genre\"].names  # ['Electronic', ..., 'Rock']\n","genre_list = [\"blues\",\"classical\",\"country\",\"disco\",\n","              \"hiphop\",\"jazz\",\"metal\",\"pop\",\"reggae\",\"rock\"]\n","GTZAN_MAP = {\n","  \"Electronic\":\"disco\",\n","  \"Experimental\":\"metal\",\n","  \"Folk\":\"country\",\n","  \"Hip-Hop\":\"hiphop\",\n","  \"Instrumental\":\"classical\",\n","  \"International\":\"reggae\",\n","  \"Pop\":\"pop\",\n","  \"Rock\":\"rock\"\n","}\n","\n","data = []\n","num_samples = len(val_ds)\n","for i in range(num_samples):\n","    try:\n","        ex = val_ds[i]\n","        waveform = ex[\"audio\"][\"array\"]\n","    except Exception as e:\n","        print(f\"Skipping sample {i} due to error: {e}\")\n","        continue\n","\n","    genre_str = fma_genres[ex[\"genre\"]]\n","    if genre_str not in GTZAN_MAP:\n","        continue\n","\n","    gtzn_lbl = GTZAN_MAP[genre_str]\n","    data.append({\"array\": waveform, \"label\": gtzn_lbl})"],"metadata":{"id":"5_hR6rJVWjnb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import WhisperModel, WhisperProcessor\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from peft import PeftModel\n","\n","num_classes = 10\n","encoder = WhisperModel.from_pretrained(\"openai/whisper-tiny\").encoder\n","model = audio_classifier(encoder, num_classes)\n","ckpt = torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/project/results/best_dense_layers_8_32.pth\", map_location=\"cpu\") #also change to ur dir\n","model.classifier.load_state_dict(ckpt)\n","\n","module_names = [name for name, module in model.encoder.named_modules()]\n","patterns = [\"layers.*.self_attn.q_proj\", \"layers.*.self_attn.k_proj\", \"layers.*.self_attn.v_proj\", \"layers.*.self_attn.o_proj\"]\n","matched_modules = []\n","for pattern in patterns:\n","    matched_modules.extend(fnmatch.filter(module_names, pattern))\n","\n","lora_config = LoraConfig(use_dora=True, r=8, lora_alpha=32, target_modules=matched_modules)\n","model.encoder = get_peft_model(model.encoder, lora_config)"],"metadata":{"id":"kvM2nwMBWmK-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_to_idx = {\n","    \"pop\": 0,\n","    \"reggae\": 1,\n","    \"country\": 2,\n","    \"rock\": 3,\n","    \"classical\": 4,\n","    \"disco\": 5,\n","    \"blues\": 6,\n","    \"hiphop\": 7,\n","    \"jazz\": 8,\n","    \"metal\": 9\n","}\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n","test_dataset = AudioDataset(data, processor, label_to_idx)\n","test_loader  = DataLoader(test_dataset,batch_size=32,shuffle=False,collate_fn=collate_fn)"],"metadata":{"id":"JUEVkPLZWpVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device).eval()\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","# 4) Run evaluation\n","metrics = evaluate(\n","    model,\n","    test_loader,\n","    device,\n","    criterion,\n",")\n","\n","print(f\"Test Loss: {metrics['loss']:.4f},   Test AUC: {metrics['auc']:.4f}\")"],"metadata":{"id":"xUSqzlZ_Wq2e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since there isn't good test datasets, the auc would be nan and loss would be higher"],"metadata":{"id":"ORIRFF77XX5m"}}]}