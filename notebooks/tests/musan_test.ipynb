{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b2fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, math, faiss, soundfile as sf\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import *\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(\n",
    "    y: np.ndarray,\n",
    "    sample_rate: int,\n",
    "    segment_sec: int = 30,\n",
    "    overlap_sec: int = 5,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"Return list of fixed-length PCM segments (zero-padded last one).\"\"\"\n",
    "    seg, hop = segment_sec * sample_rate, (segment_sec - overlap_sec) * sample_rate\n",
    "    n_segs = max(1, math.ceil((len(y) - seg) / hop) + 1)\n",
    "    segments = []\n",
    "    for i in range(n_segs):\n",
    "        s, e = i * hop, i * hop + seg\n",
    "        seg_wav = y[s:e]\n",
    "        if len(seg_wav) < seg:\n",
    "            seg_wav = np.pad(seg_wav, (0, seg - len(seg_wav)))\n",
    "        segments.append(seg_wav)\n",
    "    return segments\n",
    "\n",
    "\n",
    "def process_one(file_path: Path, out_dir: Path, sr=40_000, seg_dur=30, ovlp=5) -> int:\n",
    "    wav, _ = read_audio(str(file_path), target_fs=sr, mono=True, normalize=False)\n",
    "    y = wav.squeeze(0).cpu().numpy()\n",
    "    segments = split_audio(y, sample_rate=sr, segment_sec=seg_dur, overlap_sec=ovlp)\n",
    "    base = file_path.stem\n",
    "    for i, seg in enumerate(segments, 1):\n",
    "        out = out_dir / f\"{base}.seg{i}.wav\"\n",
    "        if not out.exists():\n",
    "            sf.write(out, seg, sr, subtype=\"PCM_16\")\n",
    "    return len(segments)\n",
    "\n",
    "\n",
    "def batch_split(\n",
    "    files: List[str],\n",
    "    output_dir: str = \"data/musan_segments\",\n",
    "    sr: int = 40_000,\n",
    "    seg_sec: int = 30,\n",
    "    overlap: int = 5,\n",
    "    workers: int = 4,\n",
    "):\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as pool:\n",
    "        tasks = [\n",
    "            pool.submit(process_one, Path(f), out_dir, sr, seg_sec, overlap)\n",
    "            for f in files\n",
    "        ]\n",
    "        total = 0\n",
    "        for fut in tqdm(tasks, desc=\"splitting\"):\n",
    "            total += fut.result()\n",
    "\n",
    "    print(f\"Done. {total} segments written to {out_dir}\")\n",
    "\n",
    "\n",
    "test_musan_full = glob.glob(\"data/musan/music/**/*.wav\", recursive=True)\n",
    "batch_split(test_musan_full, workers=os.cpu_count() // 2)\n",
    "test_musan_segment = glob.glob(\"data/musan_segments/*.wav\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13873b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_base_name(p: str) -> str:\n",
    "    stem = Path(p).stem\n",
    "    parts = stem.split(\".\")\n",
    "    return parts[-2] if len(parts) >= 2 else stem\n",
    "\n",
    "\n",
    "class MelQueryDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_paths: List[str],\n",
    "        seg_sec: int = 5,\n",
    "        n_query: int = 3,\n",
    "        sample_rate: int = 40_000,\n",
    "        window_size: int = 2_560,\n",
    "        overlap_ratio: float = 0.5,\n",
    "        n_mels: int = 256,\n",
    "    ):\n",
    "        self.paths = file_paths\n",
    "        self.window_size = window_size\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.n_mels = n_mels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.seg_len = seg_sec * self.sample_rate\n",
    "        self.n_query = n_query\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        wav, _ = read_audio(path, target_fs=self.sample_rate, mono=True)\n",
    "        wav = wav[0]\n",
    "        total = wav.size(0)\n",
    "\n",
    "        specs = []\n",
    "        for _ in range(self.n_query):\n",
    "            start = random.randint(0, max(0, total - self.seg_len))\n",
    "            seg = wav[start : start + self.seg_len]\n",
    "            spec = audio_to_melspec(\n",
    "                seg,\n",
    "                window_size=self.window_size,\n",
    "                overlap_ratio=self.overlap_ratio,\n",
    "                fs=self.sample_rate,\n",
    "                n_mels=self.n_mels,\n",
    "            )\n",
    "            specs.append(spec)\n",
    "\n",
    "        return specs, extract_base_name(path)\n",
    "\n",
    "\n",
    "class MelDocDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_paths: List[str],\n",
    "        sample_rate: int = 40_000,\n",
    "        window_size: int = 2_560,\n",
    "        overlap_ratio: float = 0.5,\n",
    "        n_mels: int = 256,\n",
    "    ):\n",
    "        self.paths = file_paths\n",
    "        self.window_size = window_size\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.n_mels = n_mels\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        wav, _ = read_audio(path, target_fs=self.sample_rate, mono=True)\n",
    "        spec = audio_to_melspec(\n",
    "            wav[0],\n",
    "            window_size=self.window_size,\n",
    "            overlap_ratio=self.overlap_ratio,\n",
    "            fs=self.sample_rate,\n",
    "            n_mels=self.n_mels,\n",
    "        )\n",
    "        return spec, extract_base_name(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ca597",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_raw = MelQueryDataset(test_musan_segment)\n",
    "doc_raw = MelDocDataset(test_musan_segment)\n",
    "\n",
    "query_cache = preprocess_and_cache_lazy(query_raw, \"data/dataset_cache/musan/query\")\n",
    "doc_cache = preprocess_and_cache_lazy(doc_raw, \"data/dataset_cache/musan/doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8777edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ds = LazyCachedDataset(\"data/dataset_cache/musan/query\")\n",
    "doc_ds = LazyCachedDataset(\"data/dataset_cache/musan/doc\")\n",
    "\n",
    "query_loader = DataLoader(\n",
    "    query_ds, batch_size=8, shuffle=False, collate_fn=lambda b: list(zip(*b))\n",
    ")\n",
    "doc_loader = DataLoader(\n",
    "    doc_ds, batch_size=8, shuffle=False, collate_fn=lambda b: list(zip(*b))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stack_to_device(xlist, device):\n",
    "    if isinstance(xlist[0], list):\n",
    "        xlist = [t for sub in xlist for t in sub]\n",
    "    return torch.stack(xlist).to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "def embed_loader(model, loader, device, is_query=False):\n",
    "    embs, names = [], []\n",
    "    with torch.no_grad():\n",
    "        for feats, fn in tqdm(loader, desc=\"embed_query\" if is_query else \"embed_doc\"):\n",
    "            feats = _stack_to_device(feats, device)\n",
    "            out = model(feats).cpu().numpy()\n",
    "            embs.append(out)\n",
    "\n",
    "            if is_query:\n",
    "                seg_per_song = feats.shape[0] // len(fn)\n",
    "                for n in fn:\n",
    "                    names.extend([n] * seg_per_song)\n",
    "            else:\n",
    "                names.extend(fn)\n",
    "\n",
    "    return np.concatenate(embs).astype(\"float32\"), names\n",
    "\n",
    "\n",
    "def process_test(model, epoch: int, top_k: int, device: torch.device):\n",
    "    ckpt = torch.load(f\"outputs/checkpoints/epoch{epoch}.pth\", map_location=\"cpu\")\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    q_emb, q_name = embed_loader(model, query_loader, device, is_query=True)\n",
    "    d_emb, d_name = embed_loader(model, doc_loader, device, is_query=False)\n",
    "\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    faiss.normalize_L2(d_emb)\n",
    "\n",
    "    index = faiss.IndexFlatIP(d_emb.shape[1])\n",
    "    index.add(d_emb)\n",
    "    _, I = index.search(q_emb, top_k * 2)\n",
    "\n",
    "    hit = 0\n",
    "    for qi, neigh in enumerate(I):\n",
    "        target = q_name[qi]\n",
    "        seen, kept = set(), 0\n",
    "        for j in neigh:\n",
    "            f = d_name[j]\n",
    "            if f not in seen:\n",
    "                seen.add(f)\n",
    "                kept += 1\n",
    "                if f == target:\n",
    "                    hit += 1\n",
    "                    break\n",
    "                if kept >= top_k:\n",
    "                    break\n",
    "\n",
    "    acc = hit / len(q_name)\n",
    "    print(f\"Epoch {epoch:02d} | recall@{top_k}: {acc:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca589d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embed_query: 100%|██████████| 83/83 [00:04<00:00, 16.84it/s]\n",
      "embed_doc: 100%|██████████| 792/792 [00:42<00:00, 18.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | recall@1: 92.6768%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResidualSTANet().to(device)\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 1, 256, 256, device=device)\n",
    "    _ = model(dummy)\n",
    "process_test(model, epoch=10, top_k=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ed045",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16, 19):\n",
    "    process_test(model, epoch=i, top_k=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099438d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
