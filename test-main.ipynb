{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wtTNYgB7ASe"
   },
   "source": [
    "# Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOezUN0L8OyP"
   },
   "outputs": [],
   "source": [
    "# Address for data cache files\n",
    "TRAIN_CACHE = \"../dataset/dataset_cache/model2/cached_train_model2.pt\"\n",
    "VAL_CACHE = \"../dataset/dataset_cache/model2/cached_val_model2.pt\"\n",
    "TEST_CACHE = \"../dataset/dataset_cache/model2/cached_test_model2.pt\"\n",
    "\n",
    "# Segment time for training and testing\n",
    "SEGMENT_TIME = 10\n",
    "\n",
    "# Number of augmented queries\n",
    "NUM_AUG_QUERIES = 3\n",
    "\n",
    "# Sampling rate\n",
    "FS = 40000\n",
    "\n",
    "# Size of the FFT window, affects frequency granularity\n",
    "WINDOW_SIZE = 2560\n",
    "\n",
    "# Ratio by which each sequential window overlaps the last and the\n",
    "# next window.\n",
    "OVERLAP_RATIO = 0.5\n",
    "\n",
    "# Number of mel layers\n",
    "N_MELS = 256\n",
    "\n",
    "# Epoch Number\n",
    "EPOCH_NUM = 100\n",
    "\n",
    "# Temperature for NT-Xent loss\n",
    "NTX_LOSS_TEM = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoUkdRSQEbj4"
   },
   "source": [
    "# Import and install libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i40hU4JLZn8_"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os, glob, random, math, gc\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f47ksxaLSV44"
   },
   "source": [
    "## Main Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugp8Wf3M-JRh"
   },
   "outputs": [],
   "source": [
    "def process_test(model, epoch, top_k):\n",
    "    model.load_state_dict(torch.load(f\"./model_cache/model3_epoch{epoch}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    test_doc_embs = []\n",
    "    test_query_embs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_loader:\n",
    "            query_feat_batch, doc_feat_batch = test_batch\n",
    "            query_feat_batch = query_feat_batch.to(device)\n",
    "            doc_feat_batch = doc_feat_batch.to(device)\n",
    "\n",
    "            query_emb = model(query_feat_batch)\n",
    "            doc_emb = model(doc_feat_batch)\n",
    "\n",
    "            test_query_embs.append(query_emb.cpu().numpy())\n",
    "            test_doc_embs.append(doc_emb.cpu().numpy())\n",
    "\n",
    "    test_query_embs = np.concatenate(test_query_embs, axis=0).astype(np.float32)\n",
    "    test_doc_embs = np.concatenate(test_doc_embs, axis=0).astype(np.float32)\n",
    "\n",
    "    dim = test_doc_embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(test_doc_embs)\n",
    "\n",
    "    D, I = index.search(test_query_embs, top_k)\n",
    "\n",
    "    TP = 0\n",
    "    num_queries = test_query_embs.shape[0]\n",
    "    for i in range(num_queries):\n",
    "        # print(f\"Query {i} Top-{top_k} Neighbors: {I[i]}\")\n",
    "        if i in I[i]:\n",
    "            TP += 1\n",
    "\n",
    "    print(f\"Epoch {epoch}: Model accuracy is {TP / num_queries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQfUFrg7gKMd"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ResNet18Model(embed_dim=EMBED_DIM, hidden_size=HIDDEN_LAYER).to(device)\n",
    "model = DimensionMaskedCNN().to(device)\n",
    "\n",
    "for epoch in range(1, 32):\n",
    "    process_test(model, epoch, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63p9upgfSYOE"
   },
   "source": [
    "## Test with ACRCloud Musics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U33-Mkz4_4og"
   },
   "outputs": [],
   "source": [
    "audio_files = glob.glob(\"../dataset/acrcloud/songkey/**/*.*\", recursive=True)\n",
    "audio_files = [f for f in audio_files if f.endswith(\".mp3\")]\n",
    "print(f\"Total audio number: {len(audio_files)}\")\n",
    "\n",
    "query_files = glob.glob(\"../dataset/acrcloud/parts2songkey/**/*.*\", recursive=True)\n",
    "query_files = [f for f in query_files if f.endswith(\".wav\")]\n",
    "print(f\"Total audio number: {len(query_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCjPk2OJSg0r"
   },
   "outputs": [],
   "source": [
    "def extract_base_name(file_path):\n",
    "    base = os.path.basename(file_path)\n",
    "    base_no_ext, _ = os.path.splitext(base)\n",
    "    parts = base_no_ext.split(\".\")\n",
    "    return parts[-1] if parts else base_no_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-VnxbuSSoT0"
   },
   "outputs": [],
   "source": [
    "class Model1TestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_paths,\n",
    "        sample_rate=FS,\n",
    "    ):\n",
    "        self.file_paths = file_paths\n",
    "        self.sample_rate = sample_rate\n",
    "        self.resize_transform = torchvision.transforms.Resize((224, 224))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.file_paths[idx]\n",
    "        channels, _ = read(file)\n",
    "        waveform = channels[0]\n",
    "\n",
    "        doc_wave = waveform\n",
    "        doc_spec = transform_to_spectrogram_stft(doc_wave)\n",
    "        doc_3ch = doc_spec.repeat(3, 1, 1)\n",
    "        doc_3ch = self.resize_transform(doc_3ch)\n",
    "\n",
    "        filename = extract_base_name(file)\n",
    "        return doc_3ch, filename\n",
    "\n",
    "\n",
    "class Model2TestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_paths,\n",
    "        sample_rate=FS,\n",
    "    ):\n",
    "        self.file_paths = file_paths\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.file_paths[idx]\n",
    "        channels, _ = read(file)\n",
    "        waveform = channels[0]\n",
    "\n",
    "        doc_wave = waveform\n",
    "        doc_spec = transform_to_spectrogram_mel(doc_wave)\n",
    "\n",
    "        filename = extract_base_name(file)\n",
    "        return doc_spec, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SE4Q7xPESxUs"
   },
   "outputs": [],
   "source": [
    "query_dataset_raw = Model2TestDataset(query_files)\n",
    "doc_dataset_raw = Model2TestDataset(audio_files)\n",
    "\n",
    "query_data = preprocess_and_cache(\n",
    "    query_dataset_raw, \"../dataset/dataset_cache/acrcloud/acrcloud_query_model2.pt\"\n",
    ")\n",
    "doc_data = preprocess_and_cache(\n",
    "    doc_dataset_raw, \"../dataset/dataset_cache/acrcloud/acrcloud_doc_model2.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bJkZR5NS2L4"
   },
   "outputs": [],
   "source": [
    "query_dataset = CachedDataset(query_data)\n",
    "doc_dataset = CachedDataset(doc_data)\n",
    "\n",
    "query_loader = DataLoader(\n",
    "    query_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "doc_loader = DataLoader(doc_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdQ11HngTG-K"
   },
   "outputs": [],
   "source": [
    "def process_test(model, epoch, top_k):\n",
    "    model.load_state_dict(torch.load(f\"./model_cache/model3_epoch{epoch}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    query_filenames = []\n",
    "    query_embs = []\n",
    "    with torch.no_grad():\n",
    "        for query_batch in query_loader:\n",
    "            query_feat, query_path = query_batch\n",
    "            query_feat = query_feat.to(device)\n",
    "            query_emb = model(query_feat)\n",
    "            query_embs.append(query_emb.cpu().numpy())\n",
    "            query_filenames.append(query_path[0])\n",
    "\n",
    "    doc_filenames = []\n",
    "    doc_embs = []\n",
    "    with torch.no_grad():\n",
    "        for doc_batch in doc_loader:\n",
    "            doc_feat, doc_path = doc_batch\n",
    "            doc_feat = doc_feat.to(device)\n",
    "            doc_emb = model(doc_feat)\n",
    "            doc_embs.append(doc_emb.cpu().numpy())\n",
    "            doc_filenames.append(doc_path[0])\n",
    "\n",
    "    query_embs = np.concatenate(query_embs, axis=0).astype(np.float32)\n",
    "    doc_embs = np.concatenate(doc_embs, axis=0).astype(np.float32)\n",
    "\n",
    "    dim = doc_embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(doc_embs)\n",
    "    print(f\"Indexed {index.ntotal} document embeddings\")\n",
    "\n",
    "    D, I = index.search(query_embs, top_k)\n",
    "\n",
    "    TP = 0\n",
    "    num_queries = query_embs.shape[0]\n",
    "    for i in range(num_queries):\n",
    "        query_base = extract_base_name(query_filenames[i])\n",
    "        retrieved_bases = [extract_base_name(doc_filenames[idx]) for idx in I[i]]\n",
    "        # print(f\"Query {i} (base: {query_base}) Top-{k} Neighbors: {retrieved_bases}\")\n",
    "        if query_base in retrieved_bases:\n",
    "            TP += 1\n",
    "\n",
    "    print(f\"Epoch {epoch}: Model accuracy is {TP / num_queries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmO97M7YTgEW"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ResNet18Model(embed_dim=EMBED_DIM, hidden_size=HIDDEN_LAYER).to(device)\n",
    "model = DimensionMaskedCNN().to(device)\n",
    "\n",
    "for epoch in range(1, 32):\n",
    "    process_test(model, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfBJ1WFqd5mi"
   },
   "source": [
    "## Test with Musan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1742844918385,
     "user": {
      "displayName": "Jiheng Li",
      "userId": "01318088611642014975"
     },
     "user_tz": 300
    },
    "id": "Bxrx-_R-d37H",
    "outputId": "2b86240b-aa70-48fd-8531-0476d75d0f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total musan music files: 660\n"
     ]
    }
   ],
   "source": [
    "test_musan_full = glob.glob(\n",
    "    \"../dataset/musan/music/**/*.*\",\n",
    "    recursive=True,\n",
    ")\n",
    "test_musan_full = [f for f in test_musan_full if f.endswith(\".wav\")]\n",
    "print(f\"Total musan music files: {len(test_musan_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RBF7_cqgMy-"
   },
   "outputs": [],
   "source": [
    "def split_audio(file_path, sample_rate, segment_duration, overlap):\n",
    "    audio, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    total_samples = len(audio)\n",
    "    seg_samples = segment_duration * sample_rate\n",
    "    step_samples = (segment_duration - overlap) * sample_rate\n",
    "\n",
    "    segments = []\n",
    "    for start in range(0, total_samples, step_samples):\n",
    "        end = start + seg_samples\n",
    "        segment = audio[start:end]\n",
    "        if len(segment) < seg_samples:\n",
    "            pad_length = seg_samples - len(segment)\n",
    "            segment = np.concatenate([segment, np.zeros(pad_length)])\n",
    "        segments.append(segment)\n",
    "        if end >= total_samples:\n",
    "            break\n",
    "    return segments\n",
    "\n",
    "output_dir = \"../dataset/musan_segments\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file_path in test_musan_full:\n",
    "    segments = split_audio(file_path, FS, segment_duration=30, overlap=5)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    for i, seg in enumerate(segments):\n",
    "        output_file = os.path.join(output_dir, f\"{base_name}.seg{i+1}.wav\")\n",
    "        sf.write(output_file, seg, FS)\n",
    "    print(f\"Processed {file_path} into {len(segments)} segments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1317,
     "status": "ok",
     "timestamp": 1742844942883,
     "user": {
      "displayName": "Jiheng Li",
      "userId": "01318088611642014975"
     },
     "user_tz": 300
    },
    "id": "GIBBUdd9jQI7",
    "outputId": "d4782739-da8c-4594-a5ab-91206bf2c136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total musan segment files: 9898\n"
     ]
    }
   ],
   "source": [
    "test_musan_segment = glob.glob(\"../dataset/musan_segments/*.*\")\n",
    "test_musan_segment = [f for f in test_musan_segment if f.endswith(\".wav\")]\n",
    "print(f\"Total musan segment files: {len(test_musan_segment)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDbry0hmhJZL"
   },
   "outputs": [],
   "source": [
    "def extract_base_name(file_path):\n",
    "    base = os.path.basename(file_path)\n",
    "    base_no_ext, _ = os.path.splitext(base)\n",
    "    parts = base_no_ext.split(\".\")\n",
    "    return parts[-2] if len(parts) >= 2 else base_no_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7vxaNYDhQL_"
   },
   "outputs": [],
   "source": [
    "class Model2QueryDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_paths,\n",
    "        segment_seconds=SEGMENT_TIME,\n",
    "        sample_rate=FS,\n",
    "        num_queries=NUM_AUG_QUERIES,\n",
    "    ):\n",
    "        self.file_paths = file_paths\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_queries = num_queries\n",
    "        self.segment_samples = int(segment_seconds * sample_rate)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.file_paths[idx]\n",
    "        channels, _ = read(file)\n",
    "        waveform = channels[0]\n",
    "        total_len = waveform.shape[0]\n",
    "\n",
    "        queries = []\n",
    "        for _ in range(self.num_queries):\n",
    "            if total_len > self.segment_samples:\n",
    "                start = random.randint(0, total_len - self.segment_samples)\n",
    "            else:\n",
    "                start = 0\n",
    "            end = start + self.segment_samples\n",
    "            query_wave = waveform[start:end]\n",
    "            query_spec = transform_to_spectrogram_mel(query_wave)\n",
    "            queries.append(query_spec)\n",
    "\n",
    "        filename = extract_base_name(file)\n",
    "        return queries, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gtp2D54hj9UK"
   },
   "outputs": [],
   "source": [
    "query_dataset_raw = Model2QueryDataset(test_musan_full)\n",
    "# Doc Dataset can just use the one from ACRCloud part\n",
    "doc_dataset_raw = Model2TestDataset(test_musan_segment)\n",
    "\n",
    "query_data = preprocess_and_cache(\n",
    "    query_dataset_raw, \"../dataset/dataset_cache/musan/musan_query_model2.pt\"\n",
    ")\n",
    "doc_data = preprocess_and_cache(\n",
    "    doc_dataset_raw, \"../dataset/dataset_cache/musan/musan_doc_model2.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QZxlmiYA8aB"
   },
   "outputs": [],
   "source": [
    "query_data = torch.load(\"../dataset/dataset_cache/musan/musan_query_model2.pt\")\n",
    "doc_data = torch.load(\"../dataset/dataset_cache/musan/musan_doc_model2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCMB4SymqXzq"
   },
   "outputs": [],
   "source": [
    "query_dataset = CachedDataset(query_data)\n",
    "doc_dataset = CachedDataset(doc_data)\n",
    "\n",
    "query_loader = DataLoader(\n",
    "    query_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "doc_loader = DataLoader(doc_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDuqKVovqrV6"
   },
   "outputs": [],
   "source": [
    "def process_test(model, epoch, top_k):\n",
    "    model.load_state_dict(torch.load(f\"./model_cache/model3_epoch{epoch}.pth\"))\n",
    "    # model.load_state_dict(torch.load(f\"./model_cache/ResNet18.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    query_filenames = []\n",
    "    query_embs = []\n",
    "    with torch.no_grad():\n",
    "        for query_batch in query_loader:\n",
    "            query_feats, query_paths = query_batch\n",
    "            for i in range(NUM_AUG_QUERIES):\n",
    "                query_feat = query_feats[i].to(device)\n",
    "                query_emb = model(query_feat)\n",
    "                query_embs.append(query_emb.cpu().numpy())\n",
    "                query_filenames.append(query_paths[0])\n",
    "\n",
    "    doc_filenames = []\n",
    "    doc_embs = []\n",
    "    with torch.no_grad():\n",
    "        for doc_batch in doc_loader:\n",
    "            doc_feat, doc_path = doc_batch\n",
    "            doc_feat = doc_feat.to(device)\n",
    "            doc_emb = model(doc_feat)\n",
    "            doc_embs.append(doc_emb.cpu().numpy())\n",
    "            doc_filenames.append(doc_path[0])\n",
    "\n",
    "    query_embs = np.concatenate(query_embs, axis=0).astype(np.float32)\n",
    "    doc_embs = np.concatenate(doc_embs, axis=0).astype(np.float32)\n",
    "\n",
    "    dim = doc_embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(doc_embs)\n",
    "\n",
    "    # D, I = index.search(query_embs, top_k)\n",
    "\n",
    "    # TP = 0\n",
    "    # num_queries = query_embs.shape[0]\n",
    "    # for i in range(num_queries):\n",
    "    #     query_base = query_filenames[i]\n",
    "    #     retrieved_bases = [doc_filenames[idx] for idx in I[i]]\n",
    "    #     # print(f\"Query {i} (base: {query_base}) Top-{k} Neighbors: {retrieved_bases}\")\n",
    "    #     if query_base in retrieved_bases:\n",
    "    #         TP += 1\n",
    "\n",
    "    # print(f\"Epoch {epoch}: Model accuracy is {TP / num_queries}\")\n",
    "\n",
    "    D, I = index.search(query_embs, top_k * 2)\n",
    "\n",
    "    TP = 0\n",
    "    num_queries = query_embs.shape[0]\n",
    "    for i in range(num_queries):\n",
    "        query_filename = query_filenames[i]\n",
    "        distinct_retrieved = []\n",
    "        for idx in I[i]:\n",
    "            candidate = doc_filenames[idx]\n",
    "            if candidate not in distinct_retrieved:\n",
    "                distinct_retrieved.append(candidate)\n",
    "            if len(distinct_retrieved) >= top_k:\n",
    "                break\n",
    "        if query_filename in distinct_retrieved:\n",
    "            TP += 1\n",
    "\n",
    "    print(f\"Epoch {epoch}: Model accuracy is {TP / num_queries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75662,
     "status": "ok",
     "timestamp": 1742854775243,
     "user": {
      "displayName": "Jiheng Li",
      "userId": "01318088611642014975"
     },
     "user_tz": 300
    },
    "id": "zmPeeBY6kvi7",
    "outputId": "bf88090c-9153-493c-be45-6ad073e6fc3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Model accuracy is 0.9904040404040404\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ResNet18Model(embed_dim=EMBED_DIM, hidden_size=HIDDEN_LAYER).to(device)\n",
    "model = DimensionMaskedCNN().to(device)\n",
    "\n",
    "for epoch in range(1, 32):\n",
    "    process_test(model, 7, 10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXsZSxQUmaCs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LoUkdRSQEbj4"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
